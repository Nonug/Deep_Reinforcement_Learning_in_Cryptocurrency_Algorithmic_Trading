{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expanded-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suburban-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Timestamp          Open          High           Low         Close  \\\n",
      "0     2014-01-01    731.903412    753.981553    731.903412    753.405481   \n",
      "1     2014-01-02    751.161970    798.243643    742.840006    784.280604   \n",
      "2     2014-01-03    782.968261    816.864470    768.646562    807.915916   \n",
      "3     2014-01-04    807.816596    828.745985    792.146837    828.745985   \n",
      "4     2014-01-05    829.028305    933.119890    828.061491    904.760075   \n",
      "...          ...           ...           ...           ...           ...   \n",
      "2552  2020-12-27  26437.037509  28288.840022  25974.002376  26388.149456   \n",
      "2553  2020-12-28  26272.294567  27357.487941  26220.911449  27098.952754   \n",
      "2554  2020-12-29  27084.807886  27324.988106  26017.620146  27324.988106   \n",
      "2555  2020-12-30  27362.436557  28928.991862  27362.436557  28827.007974   \n",
      "2556  2020-12-31  28840.953420  29244.750457  28201.991994  28988.642494   \n",
      "\n",
      "           Volume           s2f  \n",
      "0      21346662.0    113.000077  \n",
      "1      28405765.0    113.227468  \n",
      "2      31500478.0    113.370148  \n",
      "3      26004448.0    113.562091  \n",
      "4      31874485.0    113.413687  \n",
      "...           ...           ...  \n",
      "2552  192926371.0  34136.806389  \n",
      "2553  207395711.0  34380.730180  \n",
      "2554  193987680.0  34674.739210  \n",
      "2555  216061484.0  34872.246117  \n",
      "2556  211975639.0  35175.528651  \n",
      "\n",
      "[2557 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "crypto = 'btc'\n",
    "ohlc_path = os.path.join('..','data',crypto+'_price_usd_ohlc.csv')\n",
    "v_path = os.path.join('..','data',crypto+'_transactions_size_sum.csv')\n",
    "stock_to_flow_path = os.path.join('..','data',crypto+'_stock_to_flow_ratio.csv')\n",
    "df = pd.read_csv(ohlc_path)\n",
    "v = pd.read_csv(v_path)\n",
    "stock_to_flow = pd.read_csv(stock_to_flow_path)\n",
    "df['o'] = df['o'].map(lambda x:ast.literal_eval(x))\n",
    "df = df.join(pd.DataFrame(df.pop('o').values.tolist()))\n",
    "stock_to_flow['o'] = stock_to_flow['o'].map(lambda x:ast.literal_eval(x))\n",
    "stock_to_flow = stock_to_flow.join(pd.DataFrame(stock_to_flow.pop('o').values.tolist()))\n",
    "df = df.join(v['v'])\n",
    "df = df.join(stock_to_flow['ratio'])\n",
    "df = df[['t', 'o', 'h', 'l', 'c', 'v', 'ratio']]\n",
    "df.columns = ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 's2f']\n",
    "df['Volume'] = df['Volume'].astype(float)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "municipal-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for eth\n",
    "# testStartDate = '2020-01-01'\n",
    "# testEndDate = '2021-01-01'\n",
    "# testDF = df.loc[(df['Timestamp']>=testStartDate) & (df['Timestamp']<testEndDate)]\n",
    "# testDF.to_csv(os.path.join('..','data',crypto+'_'+testStartDate+'_'+testEndDate+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "challenging-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for btc train-valid-test\n",
    "# trainStartDate = \"2014-01-01\"\n",
    "# trainEndDate = \"2017-01-01\"\n",
    "# validStartDate = \"2017-01-01\"\n",
    "# validEndDate = \"2019-01-01\"\n",
    "# testStartDate = \"2019-01-01\"\n",
    "# testEndDate = \"2021-01-01\"\n",
    "# trainDF = df.loc[(df['Timestamp']>=trainStartDate) & (df['Timestamp']<trainEndDate)]\n",
    "# validDF = df.loc[(df['Timestamp']>=validStartDate) & (df['Timestamp']<validEndDate)]\n",
    "# testDF = df.loc[(df['Timestamp']>=testStartDate) & (df['Timestamp']<testEndDate)]\n",
    "# trainAndValidDF = df.loc[(df['Timestamp']>=trainStartDate) & (df['Timestamp']<validEndDate)]\n",
    "\n",
    "# trainDF.to_csv(os.path.join('..','data',crypto+'_'+trainStartDate+'_'+trainEndDate+'.csv'), index=False)\n",
    "# validDF.to_csv(os.path.join('..','data',crypto+'_'+validStartDate+'_'+validEndDate+'.csv'), index=False)\n",
    "# testDF.to_csv(os.path.join('..','data',crypto+'_'+testStartDate+'_'+testEndDate+'.csv'), index=False)\n",
    "# trainAndValidDF.to_csv(os.path.join('..','data',crypto+'_'+trainStartDate+'_'+validEndDate+'.csv'), index=False)\n",
    "# df.to_csv(os.path.join('..','data',crypto+'_'+trainStartDate+'_'+testEndDate+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "executed-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for btc cross-validation test\n",
    "\n",
    "# train\n",
    "df1 = df.loc[(df['Timestamp']>=\"2014-01-01\") & (df['Timestamp']<\"2015-01-01\")]\n",
    "df2 = df.loc[(df['Timestamp']>=\"2014-01-01\") & (df['Timestamp']<\"2016-01-01\")]\n",
    "df3 = df.loc[(df['Timestamp']>=\"2014-01-01\") & (df['Timestamp']<\"2017-01-01\")]\n",
    "df4 = df.loc[(df['Timestamp']>=\"2014-01-01\") & (df['Timestamp']<\"2018-01-01\")]\n",
    "df5 = df.loc[(df['Timestamp']>=\"2014-01-01\") & (df['Timestamp']<\"2019-01-01\")]\n",
    "\n",
    "# validate\n",
    "df6 = df.loc[(df['Timestamp']>=\"2015-01-01\") & (df['Timestamp']<\"2019-01-01\")]\n",
    "df7 = df.loc[(df['Timestamp']>=\"2016-01-01\") & (df['Timestamp']<\"2019-01-01\")]\n",
    "df8 = df.loc[(df['Timestamp']>=\"2017-01-01\") & (df['Timestamp']<\"2019-01-01\")]\n",
    "df9 = df.loc[(df['Timestamp']>=\"2018-01-01\") & (df['Timestamp']<\"2019-01-01\")]\n",
    "\n",
    "# test\n",
    "df10 = df.loc[(df['Timestamp']>=\"2020-01-01\") & (df['Timestamp']<\"2021-01-01\")]\n",
    "\n",
    "# eda\n",
    "df11 = df.loc[(df['Timestamp']>=\"2014-01-01\") & (df['Timestamp']<\"2021-01-01\")]\n",
    "\n",
    "\n",
    "df1.to_csv(os.path.join('..','data',crypto+'_'+\"2014-01-01\"+'_'+\"2015-01-01\"+'.csv'), index=False)\n",
    "df2.to_csv(os.path.join('..','data',crypto+'_'+\"2014-01-01\"+'_'+\"2016-01-01\"+'.csv'), index=False)\n",
    "df3.to_csv(os.path.join('..','data',crypto+'_'+\"2014-01-01\"+'_'+\"2017-01-01\"+'.csv'), index=False)\n",
    "df4.to_csv(os.path.join('..','data',crypto+'_'+\"2014-01-01\"+'_'+\"2018-01-01\"+'.csv'), index=False)\n",
    "df5.to_csv(os.path.join('..','data',crypto+'_'+\"2014-01-01\"+'_'+\"2019-01-01\"+'.csv'), index=False)\n",
    "df6.to_csv(os.path.join('..','data',crypto+'_'+\"2015-01-01\"+'_'+\"2019-01-01\"+'.csv'), index=False)\n",
    "df7.to_csv(os.path.join('..','data',crypto+'_'+\"2016-01-01\"+'_'+\"2019-01-01\"+'.csv'), index=False)\n",
    "df8.to_csv(os.path.join('..','data',crypto+'_'+\"2017-01-01\"+'_'+\"2019-01-01\"+'.csv'), index=False)\n",
    "df9.to_csv(os.path.join('..','data',crypto+'_'+\"2018-01-01\"+'_'+\"2019-01-01\"+'.csv'), index=False)\n",
    "df10.to_csv(os.path.join('..','data',crypto+'_'+\"2020-01-01\"+'_'+\"2021-01-01\"+'.csv'), index=False)\n",
    "df11.to_csv(os.path.join('..','data',crypto+'_'+\"2014-01-01\"+'_'+\"2021-01-01\"+'.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
